{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Using Social Media to Study the Link between Health and Happiness\n",
    "\n",
    "###Based on Twitter Data\n",
    "\n",
    "\n",
    "##A Data Science CS109 Class Project\n",
    "\n",
    "##Team: Hackers for Human Health\n",
    "Contributors\n",
    "-----\n",
    "In alphabetical order  \n",
    "\n",
    "Alejandro Covarrubias | Jacob Lurye | Eliud Oloo | Qiu-Yue Zhong\n",
    "\n",
    "\n",
    "#  \n",
    "<div style=\"float: right; margin-left: 30px;\"><img title=\"created by Stef Gibson at StefGibson.com\"style=\"float: right;margin-left: 30px;\" src=\"http://www.massage1.com/wp-content/uploads/healthhappiness.jpg\" align=right height = 350 /><figcaption>Created by Stef Gibson</figcaption></div>\n",
    "\n",
    "\n",
    "#  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#01_Motivation and Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Motivation\n",
    "------\n",
    "We are a team with diverse backgrounds in statistics, computer science, public health and biomedical research. The motivation for this project is our common interest in applying social media and data science approaches to studying and promoting human health. The scientific goal of this work was to measure average happiness state-by state using sentiment analysis of Twitter data and then to determine how well the happiness of a state correlates with public health statistics such as morbidity, mortality, healthcare quality and other criteria.\n",
    "\n",
    "\n",
    "\n",
    "##Overview\n",
    "------\n",
    "\n",
    "It is often stated that health and happiness are closely linked. Quantifiable evidence to that effect is however hard to come by.  One of many reasons for this is that happiness is an ambiguous concept -- easy to recognize in oneself but often harder to detect, much less measure, in others.  Yet, if there was ever an opportune time to make a reasonable attempt at quantifying happiness in the population, it is now.  The rapid worldwide adoption of social media platforms in recent years has tremendously increased the amount, spontaneity and frequency of human communication.   Twitter usage alone [grew six orders of magnitude](http://www.internetlivestats.com/twitter-statistics/) from 5,000 tweets per day in 2007 to 500,000,000 tweets per day in 2013. And because social media communication is recorded electronically, it has persistence - a very attractive edge over verbal communication for a data scientist.  It is remains accessible to be parsed and analyzed as new methods and expertise become available even long after the data was generated.  Studies have shown that self-disclosure in online communication is [more frequent and revealing](https://en.wikipedia.org/wiki/Self-disclosure) than face-to-face communication, presumably due to anonymity and physical distance.  Together, all these factors translate into the availability of huge volumes of data to work with in trying to gauge a phenomenon as nebulous as happiness.  A number of researchers have taken advantage of these characteristics of modern online communication and sought to measure happiness using twitter data. A notable example is the [hedonometer project by Dods *et al*](http://hedonometer.org/index.html), which attempts to measure the happiness of populations in real time. Recent hedonometer results reveal a clear spike in average happiness on United States Thanks Giving day and sharp dips that coincide with the recent terrorist shootings in Paris and San Bernardino.  Christmas Day consistently ranks as the happiest day of the year. Amusingly, the arrest of Justin Bieber in January 2014 was a pretty sad day; a testament to the the demographics of twitter users, or perhaps an indication of the the power of \"beliebers\" and the pandemic nature of \"bieber fever\".  A similar study by [Alex Davies](http://alex-davies-4lq6.squarespace.com/twitter-emoticon-meanings/gauged) sought to gauge the happiness of populations by performing sentiment analysis based on emoticons embedded in twitter messages. The study concluded that [Germans are the happiest](http://www.cam.ac.uk/research/news/germans-top-table-of-happiest-tweets) people on earth followed by Mexicans and residents of the USA.  We leveraged the experiences published by these and other researches to inform our approach in this project. Being able to reliably measure happiness opens the door to examining what the determinants of happiness in the human population are. For the current study, we chose to focus on the United States because of the ready availability of both tweet and health data for the US as well as the limitation of time and computational resources available for the project. In particular we were interested in investigating whether public health indicators released in various government and non-profit organization reports have any relationship with happiness as measured from tweets. The health data we relied on was obtained from the [American Health Rankings](http://www.americashealthrankings.org) annual report for 2014 produced by the [United Health Foundation](http://www.unitedhealthfoundation.org). The report presents a state-by-state health analysis and considers infant mortality, cancer incidences and cardiovascular deaths amongst many other factors.  \n",
    "\n",
    "Visit our project's website [here](http://hackersforhumanhealth.me).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#02_Question and Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##The question\n",
    "------\n",
    "The main question we wanted to answer is: \"Do do healthier states experience more happiness\"? To do this, we assumed that happiness as expressed in twitter communication is a surrogate for genuine happiness in the human population.  That of course is a major assumption but we felt that it is a reasonable one and that it offers as good a starting point as one can get in an effort to investigate this challenging question.  Over the course of the project, we found that, in addition to disease incidence related health statistics, the American Health Rankings annual report also considered various interesting socioeconomic and environmental parameters for each state. Some of these factors include Median Household Income, Violent Crime rates Air pollution and High School Graduation rates. We decided to modify our question to include an analysis of these factors' correlation with happiness.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had two avenues for obtaining Twitter data. The first was to collect our own data from Twitter using an API, which we did.  As a backup, we also were very fortunate to have access to a dataset of 3.5 million tweets generously provided by SÃ©bastien Gruhier of http://onemilliontweetmap.com/, to whom we are very grateful. Due to twitter usage policy, we are unable to provide a publicly acessible link to this data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both data sources, we wrote separate scripts for  parsing and reformatting the data appropriately for analysis.  This involved selecting only US tweets, adding a US state of origin label to each tweet and cleaning out unwanted characters in the tweets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "from geopy.geocoders import Nominatim\n",
    "import ast\n",
    "from time import sleep\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Data collection from Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first task was to obtain twitter data for analysis.  Since one of our intentions was to gain in harvesting tweets using the Twitter API, we wrote a python script to do just that. Please note that for security reasons, user authorization and authentication credentials have been deleted from the data collection code displayed in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from twitter import Twitter, OAuth, TwitterHTTPError, TwitterStream\n",
    "import sys\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding(\"utf-8\")\n",
    "\n",
    "# Authorization Tokens\n",
    "ckey = ''\n",
    "csecret = ''\n",
    "atoken = ''\n",
    "asecret = ''\n",
    "\n",
    "oauth = OAuth(atoken, asecret, ckey, csecret)\n",
    "\n",
    "# Initiate the connection to Twitter Streaming API\n",
    "twitter_stream = TwitterStream(auth=oauth)\n",
    "\n",
    "# Get a sample of the public data following through Twitter\n",
    "iterator = twitter_stream.statuses.filter(locations='-126,-58,26,50', lang='en')\n",
    "\n",
    "print \"\\nHow many tweets would you like to collect?\"\n",
    "tweet_count = input()\n",
    "with open('ourdata.csv','w') as tweet_file:\n",
    "\tfinal_dict = {'uid':[], 'tid':[], 'text':[], 'timestamp':[], 'city':[], 'country':[], 'bounding_box':[]}\n",
    "\tfor tweet in iterator:\n",
    "\t\ttweet_count -= 1\n",
    "\t\t# Twitter Python Tool wraps the data returned by Twitter \n",
    "\t\t# as a TwitterDictResponse object.\n",
    "\t\t# We convert it back to the JSON format to print/score\n",
    "\t\ttweet.values()\n",
    "\t\tfor k,v in tweet.iteritems():\n",
    "\t\t\tif k == 'text':\n",
    "\t\t\t\tfinal_dict['text'].append(v)\n",
    "\t\t\telif k == 'user':\n",
    "\t\t\t\tfinal_dict['uid'].append(v['id'])\n",
    "\t\t\telif k == 'id':\n",
    "\t\t\t\tfinal_dict['tid'].append(v)\n",
    "\t\t\telif k == 'timestamp_ms':\n",
    "\t\t\t\tfinal_dict['timestamp'].append(long(v))\n",
    "\t\t\telif k == 'place':\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tfinal_dict['city'].append(v['full_name'].split(',')[0])\n",
    "\t\t\t\t\tfinal_dict['country'].append(v['country'])\n",
    "\t\t\t\t\tfinal_dict['bounding_box'].append(v['bounding_box'])\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tfinal_dict['city'].append('')\n",
    "\t\t\t\t\tfinal_dict['country'].append('')\n",
    "\t\t\t\t\tfinal_dict['bounding_box'].append('')\n",
    "\n",
    "\n",
    "\t\tif tweet_count <= 0:\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\n",
    "\ttweet_df = pd.DataFrame(final_dict)\n",
    "\ttweet_df.to_csv(tweet_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample file containing tweets harvested using the above script is available [here](test.ourdata.csv)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Processing of collected tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "With the tweets collected, we went about processing the data to transform it into a format which is easier to work with. Several functions were coded to handle individual steps of the data cleaning and processing task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we write a function named find_tweet_address to derive location information based on a tweet's GPS coordinates.  For this project we were primarily interested in state information but the function can be used to get higher resolution information like county or zip codes. The function depends on the python [geopy](https://geopy.readthedocs.org/en/1.10.0/) library that offers a client for several popular geocoding web services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "geolocator = Nominatim()\n",
    "def find_tweet_address(gps_polygon_text):\n",
    "    \"\"\"\n",
    "    Get details about the location of origin of a tweet\n",
    "    based on GPS coordinates\n",
    "    \"\"\"\n",
    "    # initialize dict for storing location info\n",
    "    location_dict = None\n",
    "    gps_polygon_dict = ast.literal_eval(gps_polygon_text)\n",
    "    # get latitude and longitude\n",
    "    longitude =  gps_polygon_dict['coordinates'][0][0][0]\n",
    "    latitude =  gps_polygon_dict['coordinates'][0][0][1]\n",
    "    # grab location info from the Nominatim geocoding web service\n",
    "    tweetlocation = geolocator.reverse((latitude, longitude))\n",
    "    tweetaddress_fields = (tweetlocation.raw)\n",
    "    try:\n",
    "        county = tweetaddress_fields['address']['county']\n",
    "        state = tweetaddress_fields['address']['state']\n",
    "        zipcode = tweetaddress_fields['address']['postcode']\n",
    "    except:\n",
    "        county = ''\n",
    "        state = ''\n",
    "        zipcode = ''\n",
    "    # load location info into dict\n",
    "    location_dict = dict(county=county, state=state, zipcode=zipcode)\n",
    "    return location_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use a borrowed function to clean up the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweet_cleaner(tweet):\n",
    "    \"\"\"\n",
    "    tweet cleaning function\n",
    "    adopted from http://ravikiranj.net/posts/2012/code/how-build-twitter-sentiment-analyzer/\n",
    "    \"\"\"\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #Convert www.* or https?://* to URL\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #Convert @username to AT_USER\n",
    "    tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #trim\n",
    "    cleanedtweet = tweet.strip('\\'\"')\n",
    "    return cleanedtweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweets we collected are in a CSV file.  It is the job of the parsecsv function below to read the data file line by line, unpackage the tweets and apply the tweet text cleaning and location finder functions referenced above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parsecsv(tweet_data):\n",
    "    \"\"\"\n",
    "    parse each tweet and extract values of interest\n",
    "    \"\"\"\n",
    "    tweet_dict = None\n",
    "    # select only US tweets\n",
    "    if tweet_data[3] == \"United States\":\n",
    "        # extract data from CSV file fields\n",
    "        tweetid = tweet_data[-3]\n",
    "        userid = tweet_data[-1]\n",
    "        place = tweet_data[2]\n",
    "        coords = tweet_data[1]\n",
    "        country = tweet_data[3]\n",
    "        lang = ''\n",
    "        timestamp = tweet_data[-2]\n",
    "        ttext = tweet_data[4]\n",
    "        # clean the tweet text\n",
    "        ttext_cleand = tweet_cleaner(ttext)\n",
    "        # get location information\n",
    "        location_data = find_tweet_address(coords)\n",
    "        state = location_data['state']\n",
    "        # load extracted information into a dictionary\n",
    "        tweet_dict = dict(tweetid=tweetid, userid=userid, place=place, coords=coords, country=country, state=state, lang=lang,\n",
    "                         timestamp=timestamp, ttext=ttext, ttext_cleand=ttext_cleand)\n",
    "    else:\n",
    "        pass\n",
    "    return tweet_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our workhorse functions ready, we now define a main function to execute the workflow and write out processed output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ourdata_main():\n",
    "    \"\"\"\n",
    "    Load, reformat and clean\n",
    "     \n",
    "    \"\"\"\n",
    "    line_count = 0\n",
    "    # read input csv file contaning tweets\n",
    "    with open(incsvfile) as data_file:\n",
    "        data = csv.reader(data_file)\n",
    "        #wite output json file of processed tweet data\n",
    "        with open(outjsonfile, 'w') as fp:\n",
    "            fp.write('[' + '\\n')\n",
    "            for tweet_data in data:\n",
    "                tweet_dict = parsecsv(tweet_data)\n",
    "                out_put = json.dumps(tweet_dict)\n",
    "                if out_put != 'null':\n",
    "                    if line_count == 0:\n",
    "                        fp.write(out_put + '\\n')\n",
    "                    else:\n",
    "                        fp.write(\",\" + out_put + '\\n')\n",
    "                    line_count = 1\n",
    "            fp.write(']' + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we run our code on a small subset of the data for convenience. In practice, we ran the code using a command line version of the data cleaning and processing script shown [here](./processTwitterCsvfile.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 309 ms, sys: 21.2 ms, total: 330 ms\n",
      "Wall time: 13.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "incsvfile = ('./test.ourdata.csv')\n",
    "outjsonfile = ('test.ourdata.usa.json')\n",
    "ourdata_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting dataframe, with each row describing a single tweet and each column describing a property of that tweet, is displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coords</th>\n",
       "      <th>country</th>\n",
       "      <th>lang</th>\n",
       "      <th>place</th>\n",
       "      <th>state</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>ttext</th>\n",
       "      <th>ttext_cleand</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>userid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{u'type': u'Polygon', u'coordinates': [[[-86.3...</td>\n",
       "      <td>United States</td>\n",
       "      <td></td>\n",
       "      <td>North Muskegon</td>\n",
       "      <td></td>\n",
       "      <td>1449028922213</td>\n",
       "      <td>@westbrook_chloe @cc6163 @anniiikkkkaaa @Jenna...</td>\n",
       "      <td>AT_USER AT_USER AT_USER AT_USER AT_USER he won...</td>\n",
       "      <td>671902115011383296</td>\n",
       "      <td>531579492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{u'type': u'Polygon', u'coordinates': [[[-79.3...</td>\n",
       "      <td>United States</td>\n",
       "      <td></td>\n",
       "      <td>Mebane</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>1449028922366</td>\n",
       "      <td>My timehop is the most embarrassing thing ever...</td>\n",
       "      <td>my timehop is the most embarrassing thing ever...</td>\n",
       "      <td>671902115653120000</td>\n",
       "      <td>517114449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{u'type': u'Polygon', u'coordinates': [[[-93.2...</td>\n",
       "      <td>United States</td>\n",
       "      <td></td>\n",
       "      <td>Prien</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>1449028922422</td>\n",
       "      <td>I swea ðŸŒš he was going so fast I thought it wa...</td>\n",
       "      <td>i swea ðŸŒš he was going so fast i thought it wa...</td>\n",
       "      <td>671902115887898624</td>\n",
       "      <td>2765379648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              coords        country lang           place           state      timestamp                                              ttext                                       ttext_cleand             tweetid      userid\n",
       "0  {u'type': u'Polygon', u'coordinates': [[[-86.3...  United States       North Muskegon                  1449028922213  @westbrook_chloe @cc6163 @anniiikkkkaaa @Jenna...  AT_USER AT_USER AT_USER AT_USER AT_USER he won...  671902115011383296   531579492\n",
       "1  {u'type': u'Polygon', u'coordinates': [[[-79.3...  United States               Mebane  North Carolina  1449028922366  My timehop is the most embarrassing thing ever...  my timehop is the most embarrassing thing ever...  671902115653120000   517114449\n",
       "2  {u'type': u'Polygon', u'coordinates': [[[-93.2...  United States                Prien       Louisiana  1449028922422  I swea ðŸŒš he was going so fast I thought it wa...  i swea ðŸŒš he was going so fast i thought it wa...  671902115887898624  2765379648"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usdf = pd.read_json('test.ourdata.usa.json')\n",
    "usdf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Processing of tweets obtained from SÃ©bastien Gruhier of http://onemilliontweetmap.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our second data set of tweets was in a JSON formatted file, a sample of which is provided [here](./test.omtmdata.json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next piece of code reads in the the twitter data file and outputs another JSON file with United States tweets only, state labels added and tweet text cleaned. The online Json Viewer resource http://jsonviewer.stack.hu/ proved to be a very useful tool for easily identifying the fields to extract using our script. The viewer's rendering of a tweet in our data file looks like [this](omtm_screenshot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parsejson(tweet_data):\n",
    "    '''\n",
    "    function to parse data file and extract fields of interest\n",
    "    '''\n",
    "    # initialize dictionary for storing output\n",
    "    tweet_dict = None\n",
    "    # Filter out tweets from countries other than the US of A\n",
    "    if tweet_data[\"_source\"][\"place\"][\"country_code\"] == \"US\":\n",
    "        # extract required fields\n",
    "        tweetid = tweet_data[\"_id\"]\n",
    "        userid = tweet_data[\"_source\"][\"user\"][\"id\"]\n",
    "        place = tweet_data[\"_source\"][\"place\"][\"full_name\"]\n",
    "        coords = tweet_data[\"_source\"][\"coordinates\"]\n",
    "        country = tweet_data[\"_source\"][\"place\"][\"country\"]\n",
    "        lang = tweet_data[\"_source\"][\"lang\"]\n",
    "        timestamp = tweet_data[\"_source\"][\"timestamp_ms\"]\n",
    "        # clean the text message of tweets\n",
    "        ttext = tweet_data[\"_source\"][\"text\"]\n",
    "        ttext_cleand = tweet_cleaner(ttext)\n",
    "        state = place.strip()[-3:].strip()\n",
    "        if state == 'USA':\n",
    "            state = place.split(\",\")[0]\n",
    "        else:\n",
    "            state = state\n",
    "            \n",
    "        tweet_dict = dict(tweetid=tweetid, userid=userid, place=place, coords=coords, country=country, state=state, lang=lang,\n",
    "                         timestamp=timestamp, ttext=ttext, ttext_cleand=ttext_cleand)\n",
    "    else:\n",
    "        pass\n",
    "    return tweet_dict\n",
    " \n",
    "\n",
    "def omtmdata_main():\n",
    "    \"\"\"\n",
    "    read input tweet data, process and write output file\n",
    "    \"\"\"\n",
    "    line_count = 0\n",
    "    # load input json file for reading\n",
    "    with open(injsonfile) as data_file:\n",
    "        data = json.load(data_file)\n",
    "    # open outpt json fiel for writing\n",
    "    with open(outjsonfile, 'w') as fp:\n",
    "        fp.write('[' + '\\n')\n",
    "        # loop over tweets one by one\n",
    "        for tweet_data in data:\n",
    "            tweet_dict = parsejson(tweet_data)\n",
    "            out_put = json.dumps(tweet_dict)\n",
    "            # write parsed output\n",
    "            if out_put != 'null':\n",
    "                if line_count == 0:\n",
    "                    fp.write(out_put + '\\n')\n",
    "                else:\n",
    "                    fp.write(\",\" + out_put + '\\n')\n",
    "                line_count = 1\n",
    "        fp.write(']' + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.88 ms, sys: 1.1 ms, total: 4.98 ms\n",
      "Wall time: 45.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "injsonfile = ('./test.omtmdata.json')\n",
    "outjsonfile = ('test.omtmdata.usa.json')\n",
    "omtmdata_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we run our code in this notebook on a small subset of the data for convenience. In practice, did it using a command line version of the data cleaning and processing script linked to [here](processTwitterJsonfile.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coords</th>\n",
       "      <th>country</th>\n",
       "      <th>lang</th>\n",
       "      <th>place</th>\n",
       "      <th>state</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>ttext</th>\n",
       "      <th>ttext_cleand</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>userid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47.614937999999995,-122.3306025</td>\n",
       "      <td>United States</td>\n",
       "      <td>en</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>WA</td>\n",
       "      <td>1447266708839</td>\n",
       "      <td>After taking public transit in DC and Seattle,...</td>\n",
       "      <td>after taking public transit in dc and seattle,...</td>\n",
       "      <td>664510856407834624</td>\n",
       "      <td>537328079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37.7050435,-122.162294</td>\n",
       "      <td>United States</td>\n",
       "      <td>en</td>\n",
       "      <td>San Leandro, CA</td>\n",
       "      <td>CA</td>\n",
       "      <td>1447266710015</td>\n",
       "      <td>Thankful for all the veterans out there, I lov...</td>\n",
       "      <td>thankful for all the veterans out there, i lov...</td>\n",
       "      <td>664510861340340224</td>\n",
       "      <td>3896359752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37.7706565,-122.4359785</td>\n",
       "      <td>United States</td>\n",
       "      <td>en</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>CA</td>\n",
       "      <td>1447266716814</td>\n",
       "      <td>@Priz I've been watching, but not really enjoy...</td>\n",
       "      <td>AT_USER i've been watching, but not really enj...</td>\n",
       "      <td>664510889857433600</td>\n",
       "      <td>15532647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            coords        country lang              place state      timestamp                                              ttext                                       ttext_cleand             tweetid      userid\n",
       "0  47.614937999999995,-122.3306025  United States   en        Seattle, WA    WA  1447266708839  After taking public transit in DC and Seattle,...  after taking public transit in dc and seattle,...  664510856407834624   537328079\n",
       "1           37.7050435,-122.162294  United States   en    San Leandro, CA    CA  1447266710015  Thankful for all the veterans out there, I lov...  thankful for all the veterans out there, i lov...  664510861340340224  3896359752\n",
       "2          37.7706565,-122.4359785  United States   en  San Francisco, CA    CA  1447266716814  @Priz I've been watching, but not really enjoy...  AT_USER i've been watching, but not really enj...  664510889857433600    15532647"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usdf_omtm = pd.read_json('test.omtmdata.usa.json')\n",
    "usdf_omtm.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Health data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The American Health Rankings annual report data for 2014 was downloaded as a CSV formatted file from a dedicated website run by the United Health Foundation. A copy of the report is posted [here](AmericasHealthRankings-Annual-2014.csv). The downloaded dataset was quite clean to begin with. The data was transformed from a long format to a wide format and the value column deleted using tools available in Microsoft Excel. The resulting file, shown [here](health_cleaned.xlsx), was used for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Experiences in data acquisition and processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the experiences we faced in data acquisition and processing is that Twitter currently has a [policy](https://dev.twitter.com/rest/public/rate-limiting) in place that restricts access to historical data to paying customers via third-party vendors.  The public streaming API that we used is [rate-limited](https://dev.twitter.com/rest/public/rate-limiting) and caped to a small randomly sampled fraction of the total number of tweets available at any given moment in time.  Consequently, our data acquisition script needed to be run multiple times over several days to acquire sufficient data for analysis.  Because we did not have much time to collect data in this way, we had to resort to our backup data set from onemilliontweetmap.com. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
