{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we wanted to learn how to harvest and process twitter data, we wrote a python script to do just that: <link to script>.  As a backup, we also were very fortunate to have access to a dataset of 3.5 million tweets generously provided by SÃ©bastien Gruhier of http://onemilliontweetmap.com/ , to whom we are very grateful. Due to twitter policy, we are unable to provide a public link to this data set.\n",
    "\n",
    "For our study, we chose to focus on the United States because of the limitation of time available for the project as well as the ready availability of both tweet and health data for the US.   For both data sources, we wrote separate scripts for  parsing and reformatting the data appropriately for analysis.  This involved selecting only US tweets, adding a US state of origin label to each tweet and cleaning out unwanted characters in the tweets.  \n",
    "Some of the challenges we encountered in processing our own harvested dataset are:\n",
    "\n",
    "the onemilliontweetmap dataset (hereafter referred to omtm) were:  huge file sized and insufficient memory to process in our laptop machines.  We worked around this by using the Unix grep command to select US tweets using search term \"United States\" and  dividing the data (text files into chuncks of a million lines (tweets) using the unix head - 1000000.     These smaller files were then separately processed using the script.\n",
    "The online Json Viewer resource http://jsonviewer.stack.hu/ proved to be a very useful tool for easily identifying the fields to extract using our script.   Similarly, the Json validator online tool http://jsonviewer.stack.hu/ was helpful in ascertaining that out output files were valid Json.  \n",
    "\n",
    "out script -- getting location data -- state and county using geopy geocoded, timed out, next time use solution proposed by \n",
    "gps good if you want o be more precise street level -- use soluti0n proposed by BF on piazza\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "from geopy.geocoders import Nominatim\n",
    "import ast\n",
    "from time import sleep\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Script For Collecting Data From Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from twitter import Twitter, OAuth, TwitterHTTPError, TwitterStream\n",
    "import sys\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding(\"utf-8\")\n",
    "\n",
    "ckey = ''\n",
    "csecret = ''\n",
    "atoken = ''\n",
    "asecret = ''\n",
    "\n",
    "oauth = OAuth(atoken, asecret, ckey, csecret)\n",
    "\n",
    "# Initiate the connection to Twitter Streaming API\n",
    "twitter_stream = TwitterStream(auth=oauth)\n",
    "\n",
    "# Get a sample of the public data following through Twitter\n",
    "iterator = twitter_stream.statuses.filter(locations='-126,-58,26,50', lang='en')\n",
    "\n",
    "print \"\\nHow many tweets would you like to collect?\"\n",
    "tweet_count = input()\n",
    "with open('ourdata.csv','w') as tweet_file:\n",
    "\tfinal_dict = {'uid':[], 'tid':[], 'text':[], 'timestamp':[], 'city':[], 'country':[], 'bounding_box':[]}\n",
    "\tfor tweet in iterator:\n",
    "\t\ttweet_count -= 1\n",
    "\t\t# Twitter Python Tool wraps the data returned by Twitter \n",
    "\t\t# as a TwitterDictResponse object.\n",
    "\t\t# We convert it back to the JSON format to print/score\n",
    "\t\ttweet.values()\n",
    "\t\tfor k,v in tweet.iteritems():\n",
    "\t\t\tif k == 'text':\n",
    "\t\t\t\tfinal_dict['text'].append(v)\n",
    "\t\t\telif k == 'user':\n",
    "\t\t\t\tfinal_dict['uid'].append(v['id'])\n",
    "\t\t\telif k == 'id':\n",
    "\t\t\t\tfinal_dict['tid'].append(v)\n",
    "\t\t\telif k == 'timestamp_ms':\n",
    "\t\t\t\tfinal_dict['timestamp'].append(long(v))\n",
    "\t\t\telif k == 'place':\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tfinal_dict['city'].append(v['full_name'].split(',')[0])\n",
    "\t\t\t\t\tfinal_dict['country'].append(v['country'])\n",
    "\t\t\t\t\tfinal_dict['bounding_box'].append(v['bounding_box'])\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tfinal_dict['city'].append('')\n",
    "\t\t\t\t\tfinal_dict['country'].append('')\n",
    "\t\t\t\t\tfinal_dict['bounding_box'].append('')\n",
    "\n",
    "\n",
    "\t\tif tweet_count <= 0:\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\n",
    "\ttweet_df = pd.DataFrame(final_dict)\n",
    "\ttweet_df.to_csv(tweet_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Script for Processing Data Collected from Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "geolocator = Nominatim()\n",
    "def find_tweet_address(gps_polygon_text):\n",
    "    \"\"\"\n",
    "    Get details about the location of origin of a tweet\n",
    "    based on GPS coordinates\n",
    "    \"\"\"\n",
    "    location_dict = None\n",
    "    gps_polygon_dict = ast.literal_eval(gps_polygon_text)\n",
    "    longitude =  gps_polygon_dict['coordinates'][0][0][0]\n",
    "    latitude =  gps_polygon_dict['coordinates'][0][0][1]\n",
    "    tweetlocation = geolocator.reverse((latitude, longitude))\n",
    "    tweetaddress_fields = (tweetlocation.raw)\n",
    "    try:\n",
    "        county = tweetaddress_fields['address']['county']\n",
    "        state = tweetaddress_fields['address']['state']\n",
    "        zipcode = tweetaddress_fields['address']['postcode']\n",
    "    except:\n",
    "        county = ''\n",
    "        state = ''\n",
    "        zipcode = ''\n",
    "    location_dict = dict(county=county, state=state, zipcode=zipcode)\n",
    "    return location_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweet_cleaner(tweet):\n",
    "    \"\"\"\n",
    "    tweet cleaning function\n",
    "    adopted from http://ravikiranj.net/posts/2012/code/how-build-twitter-sentiment-analyzer/\n",
    "    \"\"\"\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #Convert www.* or https?://* to URL\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #Convert @username to AT_USER\n",
    "    tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #trim\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    return tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parsecsv(tweet_data):\n",
    "    \"\"\"\n",
    "    parse each tweet and extract values of interest\n",
    "    \"\"\"\n",
    "    tweet_dict = None\n",
    "    if tweet_data[3] == \"United States\":\n",
    "        tweetid = tweet_data[-3]\n",
    "        userid = tweet_data[-1]\n",
    "        place = tweet_data[2]\n",
    "        coords = tweet_data[1]\n",
    "        country = tweet_data[3]\n",
    "        lang = ''\n",
    "        timestamp = tweet_data[-2]\n",
    "        ttext = tweet_data[4]\n",
    "        ttext_cleand = tweet_cleaner(ttext)\n",
    "        location_data = find_tweet_address(coords)\n",
    "        state = location_data['state']\n",
    "        tweet_dict = dict(tweetid=tweetid, userid=userid, place=place, coords=coords, country=country, state=state, lang=lang,\n",
    "                         timestamp=timestamp, ttext=ttext, ttext_cleand=ttext_cleand)\n",
    "        # print \"\\n\", tweet_dict['ttext'], \"\\n\", tweet_dict['ttext_cleand'], \"\\n\", tweet_dict['state']\n",
    "    else:\n",
    "        pass\n",
    "    return tweet_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ourdata_main():\n",
    "    \"\"\"\n",
    "    Load, reformat and clean\n",
    "     \n",
    "    \"\"\"\n",
    "    line_count = 0\n",
    "    with open(incsvfile) as data_file:\n",
    "        data = csv.reader(data_file)\n",
    "        #data = data_file.xreadlines()\n",
    "        #data = data_file.read()\n",
    "        with open(outjsonfile, 'w') as fp:\n",
    "            fp.write('[' + '\\n')\n",
    "            for tweet_data in data:\n",
    "                tweet_dict = parsecsv(tweet_data)\n",
    "                out_put = json.dumps(tweet_dict)\n",
    "                if out_put != 'null':\n",
    "                    if line_count == 0:\n",
    "                        fp.write(out_put + '\\n')\n",
    "                    else:\n",
    "                        fp.write(\",\" + out_put + '\\n')\n",
    "                    line_count = 1\n",
    "            fp.write(']' + '\\n')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 460 ms, sys: 20 ms, total: 480 ms\n",
      "Wall time: 12.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "incsvfile = ('./test.ourdata.csv')\n",
    "outjsonfile = ('test.ourdata.usa.json')\n",
    "ourdata_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coords</th>\n",
       "      <th>country</th>\n",
       "      <th>lang</th>\n",
       "      <th>place</th>\n",
       "      <th>state</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>ttext</th>\n",
       "      <th>ttext_cleand</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>userid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{u'type': u'Polygon', u'coordinates': [[[-86.3...</td>\n",
       "      <td>United States</td>\n",
       "      <td></td>\n",
       "      <td>North Muskegon</td>\n",
       "      <td></td>\n",
       "      <td>1449028922213</td>\n",
       "      <td>@westbrook_chloe @cc6163 @anniiikkkkaaa @Jenna...</td>\n",
       "      <td>AT_USER AT_USER AT_USER AT_USER AT_USER he won...</td>\n",
       "      <td>671902115011383296</td>\n",
       "      <td>531579492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{u'type': u'Polygon', u'coordinates': [[[-79.3...</td>\n",
       "      <td>United States</td>\n",
       "      <td></td>\n",
       "      <td>Mebane</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>1449028922366</td>\n",
       "      <td>My timehop is the most embarrassing thing ever...</td>\n",
       "      <td>my timehop is the most embarrassing thing ever...</td>\n",
       "      <td>671902115653120000</td>\n",
       "      <td>517114449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{u'type': u'Polygon', u'coordinates': [[[-93.2...</td>\n",
       "      <td>United States</td>\n",
       "      <td></td>\n",
       "      <td>Prien</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>1449028922422</td>\n",
       "      <td>I swea ðŸŒš he was going so fast I thought it was...</td>\n",
       "      <td>i swea ðŸŒš he was going so fast i thought it was...</td>\n",
       "      <td>671902115887898624</td>\n",
       "      <td>2765379648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              coords        country lang           place           state      timestamp                                              ttext                                       ttext_cleand             tweetid      userid\n",
       "0  {u'type': u'Polygon', u'coordinates': [[[-86.3...  United States       North Muskegon                  1449028922213  @westbrook_chloe @cc6163 @anniiikkkkaaa @Jenna...  AT_USER AT_USER AT_USER AT_USER AT_USER he won...  671902115011383296   531579492\n",
       "1  {u'type': u'Polygon', u'coordinates': [[[-79.3...  United States               Mebane  North Carolina  1449028922366  My timehop is the most embarrassing thing ever...  my timehop is the most embarrassing thing ever...  671902115653120000   517114449\n",
       "2  {u'type': u'Polygon', u'coordinates': [[[-93.2...  United States                Prien       Louisiana  1449028922422  I swea ðŸŒš he was going so fast I thought it was...  i swea ðŸŒš he was going so fast i thought it was...  671902115887898624  2765379648"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usdf = pd.read_json('test.ourdata.usa.json')\n",
    "usdf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "input: twitter data file in JSON format\n",
    "output: JSON file with United States tweets only, state label added and tweet text cleaned\n",
    "\n",
    "'''\n",
    "\n",
    "#start process_tweet\n",
    "def tweetcleaner(tweet):\n",
    "    ''' function adopted from http://ravikiranj.net/posts/2012/code/how-build-twitter-sentiment-analyzer/'''\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #Convert www.* or https?://* to URL\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #Convert @username to AT_USER\n",
    "    tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #trim\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    return tweet\n",
    "#end\n",
    "\n",
    "\n",
    "def parsejson(tweet_data):\n",
    "    tweet_dict = None\n",
    "    if tweet_data[\"_source\"][\"place\"][\"country_code\"] == \"US\":\n",
    "        tweetid = tweet_data[\"_id\"]\n",
    "        userid = tweet_data[\"_source\"][\"user\"][\"id\"]\n",
    "        place = tweet_data[\"_source\"][\"place\"][\"full_name\"]\n",
    "        coords = tweet_data[\"_source\"][\"coordinates\"]\n",
    "        country = tweet_data[\"_source\"][\"place\"][\"country\"]\n",
    "        lang = tweet_data[\"_source\"][\"lang\"]\n",
    "        timestamp = tweet_data[\"_source\"][\"timestamp_ms\"]\n",
    "        ttext = tweet_data[\"_source\"][\"text\"]\n",
    "        ttext_cleand = tweetcleaner(ttext)\n",
    "        state = place.strip()[-3:].strip()\n",
    "        if state == 'USA':\n",
    "            state = place.split(\",\")[0]\n",
    "        else:\n",
    "            state = state\n",
    "            \n",
    "        tweet_dict = dict(tweetid=tweetid, userid=userid, place=place, coords=coords, country=country, state=state, lang=lang,\n",
    "                         timestamp=timestamp, ttext=ttext, ttext_cleand=ttext_cleand)\n",
    "        #print \"\\n\", tweetdict['ttext'], \"\\n\", tweetdict['ttext_cleand'], \"\\n\", tweetdict['state']\n",
    "    else:\n",
    "        pass\n",
    "    return tweet_dict\n",
    " \n",
    "\n",
    "def omtmdata_main():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    line_count = 0\n",
    "    with open(injsonfile) as data_file:\n",
    "        data = json.load(data_file)\n",
    "    with open(outjsonfile, 'w') as fp:\n",
    "        fp.write('[' + '\\n')\n",
    "        for tweet_data in data:\n",
    "            tweet_dict = parsejson(tweet_data)\n",
    "            out_put = json.dumps(tweet_dict)\n",
    "            if out_put != 'null':\n",
    "                if line_count == 0:\n",
    "                    fp.write(out_put + '\\n')\n",
    "                else:\n",
    "                    fp.write(\",\" + out_put + '\\n')\n",
    "                line_count = 1\n",
    "        fp.write(']' + '\\n')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 12.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "injsonfile = ('./test.omtmdata.json')\n",
    "outjsonfile = ('test.omtmdata.usa.json')\n",
    "omtmdata_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coords</th>\n",
       "      <th>country</th>\n",
       "      <th>lang</th>\n",
       "      <th>place</th>\n",
       "      <th>state</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>ttext</th>\n",
       "      <th>ttext_cleand</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>userid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47.614937999999995,-122.3306025</td>\n",
       "      <td>United States</td>\n",
       "      <td>en</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>WA</td>\n",
       "      <td>1447266708839</td>\n",
       "      <td>After taking public transit in DC and Seattle,...</td>\n",
       "      <td>after taking public transit in dc and seattle,...</td>\n",
       "      <td>664510856407834624</td>\n",
       "      <td>537328079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37.7050435,-122.162294</td>\n",
       "      <td>United States</td>\n",
       "      <td>en</td>\n",
       "      <td>San Leandro, CA</td>\n",
       "      <td>CA</td>\n",
       "      <td>1447266710015</td>\n",
       "      <td>Thankful for all the veterans out there, I lov...</td>\n",
       "      <td>thankful for all the veterans out there, i lov...</td>\n",
       "      <td>664510861340340224</td>\n",
       "      <td>3896359752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37.7706565,-122.4359785</td>\n",
       "      <td>United States</td>\n",
       "      <td>en</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>CA</td>\n",
       "      <td>1447266716814</td>\n",
       "      <td>@Priz I've been watching, but not really enjoy...</td>\n",
       "      <td>AT_USER i've been watching, but not really enj...</td>\n",
       "      <td>664510889857433600</td>\n",
       "      <td>15532647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            coords        country lang              place state      timestamp                                              ttext                                       ttext_cleand             tweetid      userid\n",
       "0  47.614937999999995,-122.3306025  United States   en        Seattle, WA    WA  1447266708839  After taking public transit in DC and Seattle,...  after taking public transit in dc and seattle,...  664510856407834624   537328079\n",
       "1           37.7050435,-122.162294  United States   en    San Leandro, CA    CA  1447266710015  Thankful for all the veterans out there, I lov...  thankful for all the veterans out there, i lov...  664510861340340224  3896359752\n",
       "2          37.7706565,-122.4359785  United States   en  San Francisco, CA    CA  1447266716814  @Priz I've been watching, but not really enjoy...  AT_USER i've been watching, but not really enj...  664510889857433600    15532647"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usdf_omtm = pd.read_json('test.omtmdata.usa.json')\n",
    "usdf_omtm.head(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
