{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#QUESTION\n",
    "------\n",
    "\n",
    "The main question we wanted to answer is: Do happier people generally experience better health and do healthier people experience more happiness?    To do this, we assumed that happiness as expressed in twitter communication is a surrogate for genuine happiness in the human population.  That of course is a big assumption but we felt that it is as good a starting point as you can get for investigating this very challenging problem.  Our first task was to obtain twitter data.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we wanted to learn how to harvest and process twitter data, we wrote a python script to do just that: <link to script>.  As a backup, we also were very fortunate to have access to a dataset of 3.5 million tweets generously provided by SÃ©bastien Gruhier of http://onemilliontweetmap.com/ , to whom we are very grateful. Due to twitter policy, we are unable to provide a public link to this data set.\n",
    "\n",
    "For our study, we chose to focus on the United States because of the limitation of time available for the project as well as the ready availability of both tweet and health data for the US.   For both data sources, we wrote separate scripts for  parsing and reformatting the data appropriately for analysis.  This involved selecting only US tweets, adding a US state of origin label to each tweet and cleaning out unwanted characters in the tweets.  \n",
    "Some of the challenges we encountered in processing our own harvested dataset are:\n",
    "\n",
    "the onemilliontweetmap dataset (hereafter referred to omtm) were:  huge file sized and insufficient memory to process in our laptop machines.  We worked around this by using the Unix grep command to select US tweets using search term \"United States\" and  dividing the data (text files into chuncks of a million lines (tweets) using the unix head - 1000000.     These smaller files were then separately processed using the script.\n",
    "The online Json Viewer resource http://jsonviewer.stack.hu/ proved to be a very useful tool for easily identifying the fields to extract using our script.   Similarly, the Json validator online tool http://jsonviewer.stack.hu/ was helpful in ascertaining that out output files were valid Json.  \n",
    "\n",
    "out script -- getting location data -- state and county using geopy geocoded, timed out, next time use solution proposed by \n",
    "gps good if you want o be more precise street level -- use soluti0n proposed by BF on piazza\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "from geopy.geocoders import Nominatim\n",
    "import ast\n",
    "from time import sleep\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Script For Collecting Data From Twitter"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from twitter import Twitter, OAuth, TwitterHTTPError, TwitterStream\n",
    "import sys\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding(\"utf-8\")\n",
    "\n",
    "ckey = ''\n",
    "csecret = ''\n",
    "atoken = ''\n",
    "asecret = ''\n",
    "\n",
    "oauth = OAuth(atoken, asecret, ckey, csecret)\n",
    "\n",
    "# Initiate the connection to Twitter Streaming API\n",
    "twitter_stream = TwitterStream(auth=oauth)\n",
    "\n",
    "# Get a sample of the public data following through Twitter\n",
    "iterator = twitter_stream.statuses.filter(locations='-126,-58,26,50', lang='en')\n",
    "\n",
    "print \"\\nHow many tweets would you like to collect?\"\n",
    "tweet_count = input()\n",
    "with open('tweets.csv','w') as tweet_file:\n",
    "\tfinal_dict = {'uid':[], 'tid':[], 'text':[], 'timestamp':[], 'city':[], 'country':[], 'bounding_box':[]}\n",
    "\tfor tweet in iterator:\n",
    "\t\ttweet_count -= 1\n",
    "\t\t# Twitter Python Tool wraps the data returned by Twitter \n",
    "\t\t# as a TwitterDictResponse object.\n",
    "\t\t# We convert it back to the JSON format to print/score\n",
    "\t\ttweet.values()\n",
    "\t\tfor k,v in tweet.iteritems():\n",
    "\t\t\tif k == 'text':\n",
    "\t\t\t\tfinal_dict['text'].append(v)\n",
    "\t\t\telif k == 'user':\n",
    "\t\t\t\tfinal_dict['uid'].append(v['id'])\n",
    "\t\t\telif k == 'id':\n",
    "\t\t\t\tfinal_dict['tid'].append(v)\n",
    "\t\t\telif k == 'timestamp_ms':\n",
    "\t\t\t\tfinal_dict['timestamp'].append(long(v))\n",
    "\t\t\telif k == 'place':\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tfinal_dict['city'].append(v['full_name'].split(',')[0])\n",
    "\t\t\t\t\tfinal_dict['country'].append(v['country'])\n",
    "\t\t\t\t\tfinal_dict['bounding_box'].append(v['bounding_box'])\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tfinal_dict['city'].append('')\n",
    "\t\t\t\t\tfinal_dict['country'].append('')\n",
    "\t\t\t\t\tfinal_dict['bounding_box'].append('')\n",
    "\n",
    "\n",
    "\t\tif tweet_count <= 0:\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\n",
    "\ttweet_df = pd.DataFrame(final_dict)\n",
    "\ttweet_df.to_csv(tweet_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Script for Processing Data Collected from Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "geolocator = Nominatim()\n",
    "def find_tweet_address(gps_polygon_text):\n",
    "    \"\"\"\n",
    "    Get details about the location of origin of a tweet\n",
    "    based on GPS coordinates\n",
    "    \"\"\"\n",
    "    location_dict = None\n",
    "    gps_polygon_dict = ast.literal_eval(gps_polygon_text)\n",
    "    longitude =  gps_polygon_dict['coordinates'][0][0][0]\n",
    "    latitude =  gps_polygon_dict['coordinates'][0][0][1]\n",
    "    tweetlocation = geolocator.reverse((latitude, longitude))\n",
    "    tweetaddress_fields = (tweetlocation.raw)\n",
    "    try:\n",
    "        county = tweetaddress_fields['address']['county']\n",
    "        state = tweetaddress_fields['address']['state']\n",
    "        zipcode = tweetaddress_fields['address']['postcode']\n",
    "    except:\n",
    "        county = ''\n",
    "        state = ''\n",
    "        zipcode = ''\n",
    "    location_dict = dict(county=county, state=state, zipcode=zipcode)\n",
    "    return location_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweet_cleaner(tweet):\n",
    "    \"\"\"\n",
    "    tweet cleaning function\n",
    "    adopted from http://ravikiranj.net/posts/2012/code/how-build-twitter-sentiment-analyzer/\n",
    "    \"\"\"\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #Convert www.* or https?://* to URL\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #Convert @username to AT_USER\n",
    "    tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #trim\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    return tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parsecsv(tweet_data):\n",
    "    \"\"\"\n",
    "    parse each tweet and extract values of interest\n",
    "    \"\"\"\n",
    "    tweet_dict = None\n",
    "    if tweet_data[3] == \"United States\":\n",
    "        tweetid = tweet_data[-3]\n",
    "        userid = tweet_data[-1]\n",
    "        place = tweet_data[2]\n",
    "        coords = tweet_data[1]\n",
    "        country = tweet_data[3]\n",
    "        lang = ''\n",
    "        timestamp = tweet_data[-2]\n",
    "        ttext = tweet_data[4]\n",
    "        ttext_cleand = tweet_cleaner(ttext)\n",
    "        location_data = find_tweet_address(coords)\n",
    "        state = location_data['state']\n",
    "        tweet_dict = dict(tweetid=tweetid, userid=userid, place=place, coords=coords, country=country, state=state, lang=lang,\n",
    "                         timestamp=timestamp, ttext=ttext, ttext_cleand=ttext_cleand)\n",
    "        # print \"\\n\", tweet_dict['ttext'], \"\\n\", tweet_dict['ttext_cleand'], \"\\n\", tweet_dict['state']\n",
    "    else:\n",
    "        pass\n",
    "    return tweet_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Load, reformat and clean\n",
    "     \n",
    "    \"\"\"\n",
    "    line_count = 0\n",
    "    with open(incsvfile) as data_file:\n",
    "        data = csv.reader(data_file)\n",
    "        #data = data_file.xreadlines()\n",
    "        #data = data_file.read()\n",
    "        with open(outjsonfile, 'w') as fp:\n",
    "            fp.write('[' + '\\n')\n",
    "            for tweet_data in data:\n",
    "                tweet_dict = parsecsv(tweet_data)\n",
    "                out_put = json.dumps(tweet_dict)\n",
    "                if out_put != 'null':\n",
    "                    if line_count == 0:\n",
    "                        fp.write(out_put + '\\n')\n",
    "                    else:\n",
    "                        fp.write(\",\" + out_put + '\\n')\n",
    "                    line_count = 1\n",
    "            fp.write(']' + '\\n')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 436 ms, sys: 8 ms, total: 444 ms\n",
      "Wall time: 12.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "incsvfile = ('./test.tweets.csv')\n",
    "outjsonfile = ('test.tweets.usa.json')\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coords</th>\n",
       "      <th>country</th>\n",
       "      <th>lang</th>\n",
       "      <th>place</th>\n",
       "      <th>state</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>ttext</th>\n",
       "      <th>ttext_cleand</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>userid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{u'type': u'Polygon', u'coordinates': [[[-86.3...</td>\n",
       "      <td>United States</td>\n",
       "      <td></td>\n",
       "      <td>North Muskegon</td>\n",
       "      <td></td>\n",
       "      <td>1449028922213</td>\n",
       "      <td>@westbrook_chloe @cc6163 @anniiikkkkaaa @Jenna...</td>\n",
       "      <td>AT_USER AT_USER AT_USER AT_USER AT_USER he won...</td>\n",
       "      <td>671902115011383296</td>\n",
       "      <td>531579492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{u'type': u'Polygon', u'coordinates': [[[-79.3...</td>\n",
       "      <td>United States</td>\n",
       "      <td></td>\n",
       "      <td>Mebane</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>1449028922366</td>\n",
       "      <td>My timehop is the most embarrassing thing ever...</td>\n",
       "      <td>my timehop is the most embarrassing thing ever...</td>\n",
       "      <td>671902115653120000</td>\n",
       "      <td>517114449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{u'type': u'Polygon', u'coordinates': [[[-93.2...</td>\n",
       "      <td>United States</td>\n",
       "      <td></td>\n",
       "      <td>Prien</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>1449028922422</td>\n",
       "      <td>I swea ðŸŒš he was going so fast I thought it was...</td>\n",
       "      <td>i swea ðŸŒš he was going so fast i thought it was...</td>\n",
       "      <td>671902115887898624</td>\n",
       "      <td>2765379648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              coords        country lang           place           state      timestamp                                              ttext                                       ttext_cleand             tweetid      userid\n",
       "0  {u'type': u'Polygon', u'coordinates': [[[-86.3...  United States       North Muskegon                  1449028922213  @westbrook_chloe @cc6163 @anniiikkkkaaa @Jenna...  AT_USER AT_USER AT_USER AT_USER AT_USER he won...  671902115011383296   531579492\n",
       "1  {u'type': u'Polygon', u'coordinates': [[[-79.3...  United States               Mebane  North Carolina  1449028922366  My timehop is the most embarrassing thing ever...  my timehop is the most embarrassing thing ever...  671902115653120000   517114449\n",
       "2  {u'type': u'Polygon', u'coordinates': [[[-93.2...  United States                Prien       Louisiana  1449028922422  I swea ðŸŒš he was going so fast I thought it was...  i swea ðŸŒš he was going so fast i thought it was...  671902115887898624  2765379648"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usdf = pd.read_json('test.tweets.usa.json')\n",
    "usdf.head(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
